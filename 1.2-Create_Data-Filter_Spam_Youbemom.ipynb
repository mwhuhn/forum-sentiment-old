{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Spam from Data\n",
    "Cleans the data for topic modelling\n",
    "\n",
    "## Data Sources\n",
    "- youbemom-merged.db (scraped with 1-Scrape_Forum.ipynb)\n",
    "\n",
    "\n",
    "## Changes\n",
    "- 2020-12-23: Created\n",
    "- 2021-01-18: Updated spam detection\n",
    "- 2021-01-25: Filtering spam words \n",
    "\n",
    "## TODO\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from youbemom import create_connection\n",
    "import re\n",
    "from math import floor\n",
    "from tqdm.notebook import tqdm\n",
    "from langdetect import detect\n",
    "import numpy as np\n",
    "from io import FileIO\n",
    "# saving the corpus and dictionary\n",
    "from gensim import corpora, models\n",
    "import pickle\n",
    "# topic models\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "# my functions\n",
    "from youbemom import create_connection\n",
    "from lemmatize import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old pattern = r'(http|ftp|https):\\/\\/[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "url_pattern = r'''((http|ftp|https):\\/\\/)[-a-zA-Z0-9:%\\._\\+~#=]{1,256}\\.[a-zA-Z0-9\\(\\)]{1,8}\\b([-a-zA-Z0-9<>\\*\\^\\(\\)@:%\\!,\\[\\]\\{\\}\\|'\"_\\+\\.~#\\?&/=]*)|(www\\.)*[-a-zA-Z0-9@:%\\._\\+~#=]{1,256}\\.(com|be|io|org|net)\\b([-a-zA-Z0-9<>\\*\\^\\(\\)@:%\\!,\\[\\]\\{\\}\\|'\"_\\+\\.~#\\?&/=]*)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_cutoff_pattern = r'''((http|ftp|https):\\/\\/)[-a-zA-Z0-9:%\\._\\+~#=]{1,256}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_pattern = r'([-a-zA-Z0-9_\\.\\+]+@[-a-zA-Z0-9]+\\.[-a-zA-Z0-9\\.]+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_number_pattern = r'\\b[\\+\\-x0-9]*\\d{9,}(?<!0{7})\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_pattern = r'- no subject -'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_pattern = r'[a-zA-Z]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lonely_number_pattern = r'^[0-9]+$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(conn):\n",
    "    \"\"\" gets the size of the data set in number of rows\n",
    "    :param conn: connection the the db\n",
    "    :return size: size of the posts table\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(''' SELECT COUNT(message_id) FROM posts ''')\n",
    "    size = cur.fetchone()\n",
    "    if size:\n",
    "        return int(size[0])\n",
    "    raise SystemExit(\"No size found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(chunksize):\n",
    "    \"\"\" read data in chunks from the table, format the text,\n",
    "        apply the sentiemnt analyzer, and write chunks to \n",
    "        the sentiment table\n",
    "    :param sql: selects columns of the posts table\n",
    "    :param chunksize: size of chunks\n",
    "    \"\"\"\n",
    "    sql = ''' SELECT message_id, title, body FROM posts '''\n",
    "    reader = pd.read_sql_query(sql,\n",
    "                               conn,\n",
    "                               chunksize=chunksize)\n",
    "    for i, df in enumerate(tqdm(reader)):\n",
    "        df = process_text(df)\n",
    "        df = add_spam_dummies(df)\n",
    "        df = probable_spam(df)\n",
    "        df = df[['message_id', 'text', 'text_clean', 'probable_spam']]\n",
    "        if i == 0:\n",
    "            df.to_sql('text', conn, if_exists='replace', index=False)\n",
    "        else:\n",
    "            df.to_sql('text', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text(df):\n",
    "    \"\"\" creates text column from\n",
    "        title and body\n",
    "    :param df: data frame\n",
    "    :return df: formatted data frame\n",
    "    \"\"\"\n",
    "    df['title'] = df['title'].replace('This post has been deleted\\.', '', regex=True)\n",
    "    df['text'] = df['title'] + \" \" + df['body']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_url(df):\n",
    "    \"\"\" finds urls in text strings and creates\n",
    "        new column of whether text has a url\n",
    "    :param df: data frame\n",
    "    :return df: formatted data frame\n",
    "    \"\"\"\n",
    "    regex_pat = re.compile(url_pattern, flags=re.IGNORECASE)\n",
    "    df['has_url'] = df['text'].str.contains(regex_pat)\n",
    "    regex_pat = re.compile(url_cutoff_pattern, flags=re.IGNORECASE)\n",
    "    df['has_cutoff_url'] = df['text'].str.contains(regex_pat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(df):\n",
    "    \"\"\" removes urls and cutoff urls from text strings and creates\n",
    "        new column of text without urls\n",
    "    :param df: data frame\n",
    "    :return df: formatted data frame\n",
    "    \"\"\"\n",
    "    regex_pat = re.compile(url_pattern, flags=re.IGNORECASE)\n",
    "    df['text_clean'] = df['text'].str.replace(regex_pat, \"\")\n",
    "    regex_pat = re.compile(url_cutoff_pattern, flags=re.IGNORECASE)\n",
    "    df['text_clean'] = df['text_clean'].str.replace(regex_pat, \"\")\n",
    "    df['text_clean'] = df['text_clean'].str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_subject(df):\n",
    "    \"\"\" removes - no subject - from clean text strings\n",
    "    :param df: data frame\n",
    "    :return df: formatted data frame\n",
    "    \"\"\"\n",
    "    regex_pat = re.compile(subject_pattern, flags=re.IGNORECASE)\n",
    "    df['text_clean'] = df['text_clean'].str.replace(regex_pat, \"\")\n",
    "    df['text_clean'] = df['text_clean'].str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_email(df):\n",
    "    regex_pat = re.compile(email_pattern, flags=re.IGNORECASE)\n",
    "    df['has_email'] = df['text'].str.contains(regex_pat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def has_large_number(df):\n",
    "    regex_pat = re.compile(large_number_pattern, flags=re.IGNORECASE)\n",
    "    df['has_large_number'] = df['text_clean'].str.contains(regex_pat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_alpha(df):\n",
    "    regex_pat = re.compile(alpha_pattern, flags=re.IGNORECASE)\n",
    "    df['has_alpha'] = df['text_clean'].str.contains(regex_pat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_lonely_numbers(df):\n",
    "    regex_pat = re.compile(lonely_number_pattern, flags=re.IGNORECASE)\n",
    "    df['text_clean'] = df['text_clean'].str.replace(regex_pat, \"\")\n",
    "    df['text_clean'] = df['text_clean'].str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_emptys(df):\n",
    "    df['text_clean'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text_clean'], inplace=True)\n",
    "    df.drop('title', axis=1, inplace=True)\n",
    "    df.drop('body', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_punctuation(df):\n",
    "    pattern = r'[-\\w\\s\\.,/:;!\\?\\'\\\"â€™]'\n",
    "    regex_pat = re.compile(pattern, flags=re.IGNORECASE)\n",
    "    df['n_symbols'] = df['text_clean'].str.replace(regex_pat, \"\").str.len()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_word(df, word, name=None, ignorecase=True):\n",
    "    if ignorecase:\n",
    "        regex_pat = re.compile(word.lower(), flags=re.IGNORECASE)\n",
    "    else:\n",
    "        regex_pat = re.compile(word)\n",
    "    if name:\n",
    "        df[name] = df['text'].str.contains(regex_pat)\n",
    "    else:\n",
    "        df[word] = df['text'].str.contains(regex_pat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(df):\n",
    "    df = create_text(df)\n",
    "    df = has_url(df)\n",
    "    df = remove_urls(df)\n",
    "    df = has_email(df)\n",
    "    df = has_large_number(df)\n",
    "    df = remove_no_subject(df)\n",
    "    df = count_non_punctuation(df)\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['text_clean_length'] = df['text_clean'].str.len()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spam_dummies(df):\n",
    "    for s in spam:\n",
    "        df = has_word(df, s)\n",
    "    df = has_word(df, r'\\[url', name=\"bracket_url\")\n",
    "    df = has_word(df, r'^Http', name=\"Http\", ignorecase=False)\n",
    "    df = has_word(df, r's\\.t\\.r\\.e\\.a\\.m', name=\"s.t.r.e.a.m\")\n",
    "    df = has_word(df, r'''\\bdd['s]*\\b''', name=\"has_dd\", ignorecase=False)\n",
    "    df = has_word(df, r'''\\bdh['s]*\\b''', name=\"has_dh\", ignorecase=False)\n",
    "    df = has_word(df, r'''\\bds['s]*\\b''', name=\"has_ds\", ignorecase=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probable_spam(df):\n",
    "    df['probable_spam'] = (\n",
    "        (df.vashikaran) |\n",
    "        ((~df.has_url) & df.has_large_number & df.text_length > 900) |\n",
    "        ((~df.has_url) & df.has_large_number & df.n_symbols > 10) |\n",
    "        ((~df.has_url) & df.has_large_number & df[\"problem.solution\"]) | \n",
    "        (df.has_url & df.vs & df.stream) |\n",
    "        (df.has_url & df[\"s.t.r.e.a.m\"]) |\n",
    "        (df.has_url & df.has_large_number) |\n",
    "        (df.has_url & df[\"visit.here\"]) |\n",
    "        (df.has_url & df[\"visit.at\"]) |\n",
    "        (df.has_url & df[\"amino.app\"]) |\n",
    "        (df.has_url & df[\"male.enhancement\"]) |\n",
    "        (df.has_url & df.testosterone) |\n",
    "        (df.has_url & df[\"visit.us.at\"]) |\n",
    "        (df.has_url & df[\"cbd.oil\"]) |\n",
    "        (df.has_url & df.Http) |\n",
    "        (df.has_url & df.bracket_url) |\n",
    "        (df.has_url & df.keto & df.text_length > 320) |\n",
    "        (df.has_url & df.supplement & df.text_length > 320) |\n",
    "        (df.has_url & df.pills & df.text_length > 320)\n",
    "    ) & (\n",
    "        (~df.has_dd) & (~df.has_dh) & (~df.has_ds)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get random sample of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(count, seed):\n",
    "    sql = ''' SELECT message_id\n",
    "        FROM posts\n",
    "        WHERE deleted=0\n",
    "    '''\n",
    "    conn = create_connection(path_db)\n",
    "    ids = pd.read_sql_query(sql, conn)\n",
    "    ids = ids.sample(n = count, random_state = seed)\n",
    "    temp_table_sql = ''' \n",
    "        DROP TABLE IF EXISTS temp;\n",
    "        CREATE TEMPORARY TABLE\n",
    "            temp(id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, message_id INTEGER);\n",
    "    '''\n",
    "    cur = conn.cursor()\n",
    "    cur.executescript(temp_table_sql)\n",
    "    ids.to_sql('temp', conn, if_exists='replace', index=False)\n",
    "    select_sql = '''\n",
    "        SELECT\n",
    "            p.message_id AS message_id,\n",
    "            p.title AS title,\n",
    "            p.body AS body\n",
    "        FROM posts AS p\n",
    "        WHERE p.message_id IN (SELECT message_id FROM temp)\n",
    "    '''\n",
    "    samp = pd.read_sql_query(select_sql, conn)\n",
    "    conn.close()\n",
    "    return samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database\n",
    "path_db = str(path_parent / \"database\" / \"youbemom-merged.db\")\n",
    "# spam data\n",
    "path_spam_sample = str(path_parent / \"clean_data\" / \"spam_sample_{}.txt\")\n",
    "path_spam_words = str(path_parent / \"clean_data\" / \"spam_words.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample of Data\n",
    "Code on training data set and test on test data set. Randomly select data based on seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1000000\n",
    "seed = 546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_sample(count, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = process_text(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Spam Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv(path_spam_words)\n",
    "spam = spam['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in spam:\n",
    "    sample = has_word(sample, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = has_word(sample, r'\\[url', name=\"bracket_url\")\n",
    "sample = has_word(sample, r'^Http', name=\"Http\", ignorecase=False)\n",
    "sample = has_word(sample, r's\\.t\\.r\\.e\\.a\\.m', name=\"s.t.r.e.a.m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = has_word(sample, r'''\\bdd['s]*\\b''', name=\"has_dd\", ignorecase=False)\n",
    "sample = has_word(sample, r'''\\bdh['s]*\\b''', name=\"has_dh\", ignorecase=False)\n",
    "sample = has_word(sample, r'''\\bds['s]*\\b''', name=\"has_ds\", ignorecase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample.to_csv(path_spam_sample.format(str(seed)), sep ='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded spam sample into Excel and hand-coded spam. There is minimal spam where there is no url and is always accompanied by some other indicator of spam (a long number that is probably a phone number, many non-punctuation symbols, or a specific word). I coded all urls in the first 100,000 messages in the sample, founding common key words and other idnicators. I checked this against the remaining urls in 1.2.5-Clean_Data-Identify_Spam.R, validating the spam indicators. This was used to create probable_spam function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probable Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = probable_spam(sample)\n",
    "sample['probable_spam'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "Loop through the dataframe, creating text, text_no_url, text_clean, and probable_spam in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not run above:\n",
    "spam = pd.read_csv(path_spam_words)\n",
    "spam = spam['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection(path_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = get_size(conn)\n",
    "nchunks = 100\n",
    "chunksize = floor(size / nchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e58324f5374675875e39a9d219fd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "|          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_data(chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
