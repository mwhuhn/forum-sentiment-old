{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from io import FileIO\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from lemmatize import *\n",
    "from scraping import create_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(forum=\"all\", group=\"all\", id_type=\"family_id\"):\n",
    "    lemmatized_text = pickle.load(open(path_lemma_pkl.format(forum, group, id_type), 'rb'))\n",
    "    corpus = pickle.load(open(path_corpus_pkl.format(forum, group, id_type), 'rb'))\n",
    "    dictionary = corpora.Dictionary.load(path_dictionary_gensim.format(forum, group, id_type))\n",
    "    return lemmatized_text, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cols(n):\n",
    "    return [\"topic_{}\".format(str(i).zfill(2)) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database\n",
    "path_db = str(path_parent / \"database\" / \"youbemom-merged.db\")\n",
    "# data to load\n",
    "path_lemma_pkl = str(path_parent / \"clean_data\" / \"lemmatized_text_{0}_{1}_{2}.pkl\")\n",
    "path_corpus_pkl = str(path_parent / \"clean_data\" / \"corpus_{0}_{1}_{2}.pkl\")\n",
    "path_dictionary_gensim = str(path_parent / \"clean_data\" / \"dictionary_{0}_{1}_{2}.gensim\")\n",
    "# model saving\n",
    "path_ntopic_models = str(path_parent / \"clean_data\" / \"lda_ntopics_{0}_{1}_{2}_{3}.gensim\")\n",
    "# clean text\n",
    "path_clean_text = str(path_parent / \"clean_data\" / \"clean_text_{0}_{1}.csv\")\n",
    "# topic distribution\n",
    "path_topic_counts = str(path_parent / \"clean_data\" / \"topics_{0}_{1}_{2}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'special-needs':{'n_passes':100, 'n_iterations':200, 'n_topics':[5, 10, 15, 20, 25, 30, 40, 50]},\n",
    "    'tween-teen':   {'n_passes':120, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30, 40, 50]},\n",
    "    'preschool':    {'n_passes':100, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30]},\n",
    "    'elementary':   {'n_passes':130, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30]},\n",
    "    'new-york-city':{'n_passes':150, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30, 40, 50]},\n",
    "    'school':       {'n_passes':200, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30, 40, 50]},\n",
    "    'toddler':      {'n_passes':35,  'n_iterations':200, 'n_topics':[5, 10, 15, 20, 25, 30, 40, 50]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'all'\n",
    "id_type = 'family_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = \"special-needs\"\n",
    "n_sn = 20\n",
    "lemmatized_text_sn, corpus_sn, dictionary_sn = load_data(sf, group, id_type)\n",
    "mod_sn = LdaModel.load(path_ntopic_models.format(sf, group, id_type, str(n_sn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_sn = pd.read_csv(path_clean_text.format(sf, group))\n",
    "ct_sn = ct_sn[['family_id','text_clean']].groupby(['family_id'])['text_clean'].apply(' '.join)\n",
    "ct_sn = pd.DataFrame(ct_sn)\n",
    "ct_sn.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf = \"toddler\"\n",
    "n_td = 15\n",
    "lemmatized_text_td, corpus_td, dictionary_td = load_data(sf, group, id_type)\n",
    "mod_td = LdaModel.load(path_ntopic_models.format(sf, group, id_type, str(n_td)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_td = pd.read_csv(path_clean_text.format(sf, group))\n",
    "ct_td = ct_td[['family_id','text_clean']].groupby(['family_id'])['text_clean'].apply(' '.join)\n",
    "ct_td = pd.DataFrame(ct_td)\n",
    "ct_td.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toddler topic distribution on special-needs documents\n",
    "## must make new corpus for special-needs using toddler dictionary so\n",
    "## the lda model matches words correctly\n",
    "corpus_sn_using_td = [dictionary_td.doc2bow(t) for t in lemmatized_text_sn]\n",
    "out_sn = np.zeros((len(corpus_sn_using_td), n_td), dtype=float, order='C')\n",
    "for i, doc in enumerate(corpus_sn_using_td):\n",
    "    topics = mod_td.get_document_topics(doc)\n",
    "    for j, score in topics:\n",
    "        out_sn[i,j] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df_sn = pd.concat([ct_sn, pd.DataFrame(out_sn, columns=gen_cols(n_td))], axis=1)\n",
    "topic_df_sn.to_csv(path_topic_counts.format(\"special-needs\", group, \"using_td_15\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out_sn)\n",
    "colsums = df.sum()\n",
    "totalsum = colsums.sum()\n",
    "100 * colsums / totalsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special-needs topic distribution on toddler documents\n",
    "corpus_td_using_sn = [dictionary_sn.doc2bow(t) for t in lemmatized_text_td]\n",
    "out_td = np.zeros((len(corpus_td_using_sn), n_sn), dtype=float, order='C')\n",
    "for i, doc in enumerate(corpus_td_using_sn):\n",
    "    topics = mod_sn.get_document_topics(doc)\n",
    "    for j, score in topics:\n",
    "        out_td[i,j] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df_td = pd.concat([ct_td, pd.DataFrame(out_td, columns=gen_cols(n_sn))], axis=1)\n",
    "topic_df_td.to_csv(path_topic_counts.format(\"toddler\", group, \"using_sn_20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out_td)\n",
    "colsums = df.sum()\n",
    "totalsum = colsums.sum()\n",
    "100 * colsums / totalsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# school-age corpus\n",
    "sf = \"school\"\n",
    "lemmatized_text_sa, corpus_sa, dictionary_sa = load_data(sf, group, id_type)\n",
    "corpus_sa_using_sn = [dictionary_sn.doc2bow(t) for t in lemmatized_text_sa]\n",
    "out_sa = np.zeros((len(corpus_sa_using_sn), n_sn), dtype=float, order='C')\n",
    "for i, doc in enumerate(corpus_sa_using_sn):\n",
    "    topics = mod_sn.get_document_topics(doc)\n",
    "    for j, score in topics:\n",
    "        out_sa[i,j] = score\n",
    "ct_sa = pd.read_csv(path_clean_text.format(sf, group))\n",
    "ct_sa = ct_sa[['family_id','text_clean']].groupby(['family_id'])['text_clean'].apply(' '.join)\n",
    "ct_sa = pd.DataFrame(ct_sa)\n",
    "ct_sa.reset_index(inplace=True)\n",
    "topic_df_sa = pd.concat([ct_sa, pd.DataFrame(out_sa, columns=gen_cols(n_sn))], axis=1)\n",
    "topic_df_sa.to_csv(path_topic_counts.format(\"school\", group, \"using_sn_20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df_sa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find word frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_sn = pd.read_csv(path_clean_text.format(\"special-needs\", group))\n",
    "ct_td = pd.read_csv(path_clean_text.format(\"toddler\", group))\n",
    "ct_sa = pd.read_csv(path_clean_text.format(\"school\", group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_SN = ct_sn.shape[0]\n",
    "LEN_TD = ct_td.shape[0]\n",
    "LEN_SA = ct_sa.shape[0]\n",
    "def token_freq(word):\n",
    "    sn_freq = ct_sn['text_clean'].str.count(word).sum() / LEN_SN\n",
    "    td_freq = ct_td['text_clean'].str.count(word).sum() / LEN_TD\n",
    "    sa_freq = ct_sa['text_clean'].str.count(word).sum() / LEN_SA\n",
    "    return {'special-needs': sn_freq, 'toddler': td_freq, 'school': sa_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_rates(token1, token2):\n",
    "    freq1 = token_freq(token1)\n",
    "    freq2 = token_freq(token2)\n",
    "    rates = {\n",
    "        'special-needs': freq1['special-needs'] / freq2['special-needs'],\n",
    "        'toddler': freq1['toddler'] / freq2['toddler'],\n",
    "        'school': freq1['school'] / freq2['school']\n",
    "    }\n",
    "    print(\"word 1\")\n",
    "    print(\"special-needs: \", freq1['special-needs'])\n",
    "    print(\"toddler:       \", freq1['toddler'])\n",
    "    print(\"school:        \", freq1['school'])\n",
    "    print(\"word 2\")\n",
    "    print(\"special-needs: \", freq2['special-needs'])\n",
    "    print(\"toddler:       \", freq2['toddler'])\n",
    "    print(\"school:        \", freq2['school'])\n",
    "    print(\"relative rate\")\n",
    "    print(\"special-needs: \", rates['special-needs'])\n",
    "    print(\"toddler:       \", rates['toddler'])\n",
    "    print(\"school:        \", rates['school'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_rates(word):\n",
    "    freq = token_freq(word)\n",
    "    print(\"word:          \", word)\n",
    "    print(\"special-needs: \", freq['special-needs'])\n",
    "    print(\"toddler:       \", freq['toddler'])\n",
    "    print(\"school:        \", freq['school'])\n",
    "#     if freq['special-needs'] / freq['toddler'] >= 1:\n",
    "#         print(\"sn / td:       \", freq['special-needs'] / freq['toddler'])\n",
    "#     else:\n",
    "#         print(\"td / sn:       \", freq['toddler'] / freq['special-needs'])\n",
    "#     if freq['special-needs'] / freq['school'] >= 1:\n",
    "#         print(\"sn / sa:       \", freq['special-needs'] / freq['school'])\n",
    "#     else:\n",
    "#         print(\"sa / sn:       \", freq['school'] / freq['special-needs'])\n",
    "    print(\"sn / td:       \", freq['special-needs'] / freq['toddler'])\n",
    "    print(\"sn / sa:       \", freq['special-needs'] / freq['school'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\beasy\\b\n",
      "special-needs:  0.005371738006673101\n",
      "toddler:        0.003559422358156463\n",
      "school:         0.0042467948717948715\n",
      "sn / td:        1.5091600451302705\n",
      "sn / sa:        1.2648922702505718\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\beasy\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\bhard\\b\n",
      "special-needs:  0.029465031286939735\n",
      "toddler:        0.008981736938193194\n",
      "school:         0.015865384615384615\n",
      "sn / td:        3.2805493513893818\n",
      "sn / sa:        1.8571898508131712\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\bhard\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 1\n",
      "special-needs:  0.029465031286939735\n",
      "toddler:        0.008981736938193194\n",
      "school:         0.015865384615384615\n",
      "word 2\n",
      "special-needs:  0.005371738006673101\n",
      "toddler:        0.003559422358156463\n",
      "school:         0.0042467948717948715\n",
      "relative rate\n",
      "special-needs:  5.485195154777927\n",
      "toddler:        2.5233692533316328\n",
      "school:         3.7358490566037736\n"
     ]
    }
   ],
   "source": [
    "token_rates(\"\\\\bhard\\\\b\", \"\\\\beasy\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_rates(\"\\\\bhas[\\sa\\s|\\s]disab*\", \"\\\\bis\\sdisab*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_rates(\"\\\\bhas\\\\s[add|adhd]\\\\b\", \"\\\\bis\\\\s[add|adhd]\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_rates(\"\\\\bhas\\\\sautis*\", \"\\\\bis\\\\sautis*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 1\n",
      "special-needs:  0.0002277385561375541\n",
      "toddler:        0.00015932630845028612\n",
      "school:         0.00016025641025641026\n",
      "word 2\n",
      "special-needs:  5.78383634635058e-05\n",
      "toddler:        8.624501027223464e-05\n",
      "school:         0.00016025641025641026\n",
      "relative rate\n",
      "special-needs:  3.9375000000000004\n",
      "toddler:        1.8473684210526318\n",
      "school:         1.0\n"
     ]
    }
   ],
   "source": [
    "token_rates(\"retarded\\\\b\", \"retard\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\bspecial[\\s-]need.\\b\n",
      "special-needs:  0.00712857829687709\n",
      "toddler:        0.00017634834995138506\n",
      "school:         0.0032852564102564103\n",
      "sn / td:        40.42327755741557\n",
      "sn / sa:        2.1698696864640508\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\bspecial[\\s-]need.\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\bhas\\sautis.*\n",
      "special-needs:  0.0009290287131325619\n",
      "toddler:        2.882399027519421e-05\n",
      "school:         0.0002403846153846154\n",
      "sn / td:        32.23109306736339\n",
      "sn / sa:        3.8647594466314574\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\bhas\\\\sautis.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\bis\\sautis*\n",
      "special-needs:  0.0003795642602292568\n",
      "toddler:        1.7929883714490888e-05\n",
      "school:         0.0\n",
      "sn / td:        21.169365416603004\n",
      "sn / sa:        inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-ff3c02d34fd9>:11: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if freq['special-needs'] / freq['school'] >= 1:\n",
      "<ipython-input-14-ff3c02d34fd9>:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(\"sn / sa:       \", freq['special-needs'] / freq['school'])\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\bis\\\\sautis.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           retarded\\b\n",
      "special-needs:  0.0002277385561375541\n",
      "toddler:        0.00015932630845028612\n",
      "school:         0.00016025641025641026\n",
      "sn / td:        1.429384502488579\n",
      "sn / sa:        1.4210885902983375\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"retarded\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           retard\\b\n",
      "special-needs:  5.78383634635058e-05\n",
      "toddler:        8.624501027223464e-05\n",
      "school:         0.00016025641025641026\n",
      "td / sn:        1.491138495414943\n",
      "sa / sn:        2.7707632211538464\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"retard\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "board_rates(\"\\\\bmoron\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_rates(\"\\\\bimbecile\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_rates(\"\\\\bhandicapped\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_rates(\"\\\\bdisabled\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_rates(\"\\\\bidiot\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\bsn\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\bsad\\b\n",
      "special-needs:  0.004843962940068611\n",
      "toddler:        0.004259368704681652\n",
      "school:         0.005689102564102564\n",
      "sn / td:        1.1372490328776672\n",
      "sn / sa:        0.8514458801698066\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\bsad\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\bguilt.*\\b\n",
      "special-needs:  0.0011965311441512762\n",
      "toddler:        0.0011477395182807647\n",
      "school:         0.0015224358974358974\n",
      "sn / td:        1.042511062042717\n",
      "sn / sa:        0.7859320357372593\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\bguilt.*\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\b[anxiety|anxious]\\b\n",
      "special-needs:  1.2579880202289677\n",
      "toddler:        0.7698681268400827\n",
      "school:         0.9633012820512821\n",
      "sn / td:        1.6340305259712065\n",
      "sn / sa:        1.3059133665328162\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\b[anxiety|anxious]\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\bdepressed\\b\n",
      "special-needs:  0.0017279211084722359\n",
      "toddler:        0.0008261364141866687\n",
      "school:         0.003205128205128205\n",
      "sn / td:        2.091568751600635\n",
      "sa / sn:        1.854904248471194\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\bdepressed\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:           \\bmiserable\\b\n",
      "special-needs:  0.0016592380518593226\n",
      "toddler:        0.0008626770632756944\n",
      "school:         0.0015224358974358974\n",
      "sn / td:        1.9233594151199347\n",
      "sn / sa:        1.089857415116018\n"
     ]
    }
   ],
   "source": [
    "board_rates(\"\\\\bmiserable\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_freq(\"\\\\bneed\\shelp\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\bhelp\\sme\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\biep\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\b504\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_freq(\"\\\\bds\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\bdd\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\bdh\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\bschool\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\bdoctor\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\btherapist\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(\"\\\\basd\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer()\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "l_sn = [\" \".join(l) for l in lemmatized_text_sn]\n",
    "countvec_sn = countvectorizer.fit_transform(l_sn)\n",
    "tfidfvec_sn = tfidfvectorizer.fit_transform(l_sn)\n",
    "count_tokens_sn = countvectorizer.get_feature_names()\n",
    "tfidf_tokens_sn = tfidfvectorizer.get_feature_names()\n",
    "df_countvec_sn = pd.DataFrame(data = countvec_sn.toarray(),columns = count_tokens_sn)\n",
    "df_tfidfvec_sn = pd.DataFrame(data = tfidfvec_sn.toarray(),columns = tfidf_tokens_sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer()\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "l_td = [\" \".join(l) for l in lemmatized_text_td]\n",
    "countvec_td = countvectorizer.fit_transform(l_td)\n",
    "tfidfvec_td = tfidfvectorizer.fit_transform(l_td)\n",
    "count_tokens_td = countvectorizer.get_feature_names()\n",
    "tfidf_tokens_td = tfidfvectorizer.get_feature_names()\n",
    "df_countvec_td = pd.DataFrame(data = countvec_td.toarray(),columns = count_tokens_td)\n",
    "df_tfidfvec_td = pd.DataFrame(data = tfidfvec_td.toarray(),columns = tfidf_tokens_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
