{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from io import FileIO\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from lemmatize import *\n",
    "from scraping import create_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwh/miniconda3/envs/forum/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(forum=\"all\", group=\"all\", id_type=\"family_id\"):\n",
    "    lemmatized_text = pickle.load(open(path_lemma_pkl.format(forum, group, id_type), 'rb'))\n",
    "    corpus = pickle.load(open(path_corpus_pkl.format(forum, group, id_type), 'rb'))\n",
    "    dictionary = corpora.Dictionary.load(path_dictionary_gensim.format(forum, group, id_type))\n",
    "    return lemmatized_text, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cols(n):\n",
    "    return [\"topic_{}\".format(str(i).zfill(2)) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database\n",
    "path_db = str(path_parent / \"database\" / \"youbemom-merged.db\")\n",
    "# data to load\n",
    "path_lemma_pkl = str(path_parent / \"clean_data\" / \"lemmatized_text_{0}_{1}_{2}.pkl\")\n",
    "path_corpus_pkl = str(path_parent / \"clean_data\" / \"corpus_{0}_{1}_{2}.pkl\")\n",
    "path_dictionary_gensim = str(path_parent / \"clean_data\" / \"dictionary_{0}_{1}_{2}.gensim\")\n",
    "# model saving\n",
    "path_ntopic_models = str(path_parent / \"clean_data\" / \"lda_ntopics_{0}_{1}_{2}_{3}.gensim\")\n",
    "# clean text\n",
    "path_clean_text = str(path_parent / \"clean_data\" / \"clean_text_{0}_{1}.csv\")\n",
    "# topic distribution\n",
    "path_topic_counts = str(path_parent / \"clean_data\" / \"topics_{0}_{1}_{2}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'special-needs':{'n_passes':100, 'n_iterations':200, 'n_topics':[5, 10, 15, 20, 25, 30, 40, 50]},\n",
    "    'tween-teen':   {'n_passes':120, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30, 40, 50]},\n",
    "    'preschool':    {'n_passes':100, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30]},\n",
    "    'elementary':   {'n_passes':130, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30]},\n",
    "    'new-york-city':{'n_passes':150, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30, 40, 50]},\n",
    "    'school':       {'n_passes':200, 'n_iterations':400, 'n_topics':[2, 3, 4, 5, 7, 10, 15, 20, 25, 30, 40, 50]},\n",
    "    'toddler':      {'n_passes':35,  'n_iterations':200, 'n_topics':[5, 10, 15, 20, 25, 30, 40, 50]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'all'\n",
    "id_type = 'family_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = \"special-needs\"\n",
    "n_sn = 20\n",
    "lemmatized_text_sn, corpus_sn, dictionary_sn = load_data(sf, group, id_type)\n",
    "mod_sn = LdaModel.load(path_ntopic_models.format(sf, group, id_type, str(n_sn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_sn = pd.read_csv(path_clean_text.format(sf, group))\n",
    "ct_sn = ct_sn[['family_id','text_clean']].groupby(['family_id'])['text_clean'].apply(' '.join)\n",
    "ct_sn = pd.DataFrame(ct_sn)\n",
    "ct_sn.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf = \"toddler\"\n",
    "n_td = 15\n",
    "lemmatized_text_td, corpus_td, dictionary_td = load_data(sf, group, id_type)\n",
    "mod_td = LdaModel.load(path_ntopic_models.format(sf, group, id_type, str(n_td)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_td = pd.read_csv(path_clean_text.format(sf, group))\n",
    "ct_td = ct_td[['family_id','text_clean']].groupby(['family_id'])['text_clean'].apply(' '.join)\n",
    "ct_td = pd.DataFrame(ct_td)\n",
    "ct_td.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toddler topic distribution on special-needs documents\n",
    "## must make new corpus for special-needs using toddler dictionary so\n",
    "## the lda model matches words correctly\n",
    "corpus_sn_using_td = [dictionary_td.doc2bow(t) for t in lemmatized_text_sn]\n",
    "out_sn = np.zeros((len(corpus_sn_using_td), n_td), dtype=float, order='C')\n",
    "for i, doc in enumerate(corpus_sn_using_td):\n",
    "    topics = mod_td.get_document_topics(doc)\n",
    "    for j, score in topics:\n",
    "        out_sn[i,j] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df_sn = pd.concat([ct_sn, pd.DataFrame(out_sn, columns=gen_cols(n_td))], axis=1)\n",
    "topic_df_sn.to_csv(path_topic_counts.format(\"special-needs\", group, \"using_td_15\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.216413\n",
       "1      0.735056\n",
       "2     51.257636\n",
       "3      0.148091\n",
       "4      2.125724\n",
       "5      2.988512\n",
       "6      3.425553\n",
       "7     10.825894\n",
       "8      0.176394\n",
       "9      0.648541\n",
       "10     0.531523\n",
       "11     3.281950\n",
       "12    23.235781\n",
       "13     0.254778\n",
       "14     0.148152\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(out_sn)\n",
    "colsums = df.sum()\n",
    "totalsum = colsums.sum()\n",
    "100 * colsums / totalsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special-needs topic distribution on toddler documents\n",
    "corpus_td_using_sn = [dictionary_sn.doc2bow(t) for t in lemmatized_text_td]\n",
    "out_td = np.zeros((len(corpus_td_using_sn), n_sn), dtype=float, order='C')\n",
    "for i, doc in enumerate(corpus_td_using_sn):\n",
    "    topics = mod_sn.get_document_topics(doc)\n",
    "    for j, score in topics:\n",
    "        out_td[i,j] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df_td = pd.concat([ct_td, pd.DataFrame(out_td, columns=gen_cols(n_sn))], axis=1)\n",
    "topic_df_td.to_csv(path_topic_counts.format(\"toddler\", group, \"using_sn_20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.050317\n",
       "1      0.370314\n",
       "2      5.466853\n",
       "3      0.432776\n",
       "4      0.551899\n",
       "5      2.679955\n",
       "6     11.084513\n",
       "7      2.983304\n",
       "8      0.112575\n",
       "9     28.373404\n",
       "10     3.541574\n",
       "11     0.460947\n",
       "12     0.155415\n",
       "13     0.235904\n",
       "14     1.319802\n",
       "15     1.779726\n",
       "16     0.677381\n",
       "17     0.337585\n",
       "18    38.019294\n",
       "19     0.366460\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(out_td)\n",
    "colsums = df.sum()\n",
    "totalsum = colsums.sum()\n",
    "100 * colsums / totalsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find word frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_sn = pd.read_csv(path_clean_text.format(\"special-needs\", group))\n",
    "ct_td = pd.read_csv(path_clean_text.format(\"toddler\", group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_SN = ct_sn.shape[0]\n",
    "LEN_TD = ct_td.shape[0]\n",
    "def token_counts(word):\n",
    "    sn_freq = ct_sn['text_clean'].str.count(word).sum() / LEN_SN\n",
    "    td_freq = ct_td['text_clean'].str.count(word).sum() / LEN_TD\n",
    "    print(\"special-needs:\", sn_freq)\n",
    "    print(\"toddler:      \", td_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special-needs: 0.00025304284015283785\n",
      "toddler:       1.6795080947750958e-05\n"
     ]
    }
   ],
   "source": [
    "token_counts(\"\\\\bis\\sautistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special-needs: 0.0009290287131325619\n",
      "toddler:       2.9050950828542194e-05\n"
     ]
    }
   ],
   "source": [
    "token_counts(\"\\\\bhas\\sautism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7297297297297296"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.9050950828542194e-05 / 1.6795080947750958e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6714285714285717"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0009290287131325619 / 0.00025304284015283785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special-needs: 5.78383634635058e-05\n",
      "toddler:       8.624501027223464e-05\n"
     ]
    }
   ],
   "source": [
    "token_counts(\"retard\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special-needs: 0.0002277385561375541\n",
      "toddler:       0.00015932630845028612\n"
     ]
    }
   ],
   "source": [
    "token_counts(\"retarded\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special-needs: 0.07336434915574064\n",
      "toddler:       0.0006733919617834742\n"
     ]
    }
   ],
   "source": [
    "token_counts(\"\\\\bsn\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special-needs: 0.0011025438035230793\n",
      "toddler:       0.0002589619913700519\n"
     ]
    }
   ],
   "source": [
    "token_counts(\"\\\\bneed\\shelp\\\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer()\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "l_sn = [\" \".join(l) for l in lemmatized_text_sn]\n",
    "countvec_sn = countvectorizer.fit_transform(l_sn)\n",
    "tfidfvec_sn = tfidfvectorizer.fit_transform(l_sn)\n",
    "count_tokens_sn = countvectorizer.get_feature_names()\n",
    "tfidf_tokens_sn = tfidfvectorizer.get_feature_names()\n",
    "df_countvec_sn = pd.DataFrame(data = countvec_sn.toarray(),columns = count_tokens_sn)\n",
    "df_tfidfvec_sn = pd.DataFrame(data = tfidfvec_sn.toarray(),columns = tfidf_tokens_sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer()\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "l_td = [\" \".join(l) for l in lemmatized_text_td]\n",
    "countvec_td = countvectorizer.fit_transform(l_td)\n",
    "tfidfvec_td = tfidfvectorizer.fit_transform(l_td)\n",
    "count_tokens_td = countvectorizer.get_feature_names()\n",
    "tfidf_tokens_td = tfidfvectorizer.get_feature_names()\n",
    "df_countvec_td = pd.DataFrame(data = countvec_td.toarray(),columns = count_tokens_td)\n",
    "df_tfidfvec_td = pd.DataFrame(data = tfidfvec_td.toarray(),columns = tfidf_tokens_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
