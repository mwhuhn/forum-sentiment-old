{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sentiment Analysis on Forum Data\n",
    "This notebook loads the Youbemom forum data and calculates sentiment\n",
    "\n",
    "## Data Sources\n",
    "- youbemom-merged.db (created with 1.1-Merge_Databases.ipynb)\n",
    "\n",
    "## Changes\n",
    "- 2020-08-13: Set up data cleaning\n",
    "- 2020-08-20: Added t-tests\n",
    "- 2020-08-26: Added plots\n",
    "- 2020-09-14: Added more plots\n",
    "- 2020-09-15: Compared parent and child sentiment\n",
    "- 2020-12-10: Changed data set\n",
    "- 2020-12-13: Moved data analysis to new file\n",
    "- 2020-12-15: Created new sentiment table, removed urls from strings\n",
    "- 2021-01-07: Chunked data analysis into loop\n",
    "- 2021-01-25: Moved text creation and cleaning to 1.2-Create_Data-Filter_Spam\n",
    "\n",
    "## Database Structure\n",
    "- threads\n",
    " - id: automatically assigned\n",
    " - url: url of top post\n",
    " - subforum: subforum of post\n",
    " - dne: post does not exist\n",
    "- posts\n",
    " - id: automatically assigned\n",
    " - family_id: thread->id\n",
    " - message_id: the unique id of the message from the html\n",
    " - parent_id: id of post this post is responding to, 0 if top post\n",
    " - date_recorded: date the data is fetched\n",
    " - date_created: date the data was created\n",
    " - title: title of the post\n",
    " - body: body of the post\n",
    " - subforum: subforum of post\n",
    " - deleted: has post been deleted\n",
    "- text\n",
    " - message_id: message id connecting to posts\n",
    " - text: title + body\n",
    " - text_clean: text without urls and extra spaces\n",
    " - probable_spam: marked as probable spam in 1.2\n",
    " - neg_sen_all\n",
    " - neu_sen_all\n",
    " - pos_sen_all\n",
    " - com_sen_all\n",
    " - neg_sen_no_url\n",
    " - neu_sen_no_url\n",
    " - pos_sen_no_url\n",
    " - com_sen_no_url\n",
    "\n",
    "## TODO\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scraping import create_connection\n",
    "import re\n",
    "from math import floor\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "For fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(conn):\n",
    "    \"\"\" gets the size of the data set in number of rows\n",
    "    :param conn: connection the the db\n",
    "    :return size: size of the posts table\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(''' SELECT COUNT(message_id) FROM posts ''')\n",
    "    size = cur.fetchone()\n",
    "    if size:\n",
    "        return int(size[0])\n",
    "    raise SystemExit(\"No size found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(chunksize):\n",
    "    \"\"\" read data in chunks from the table, format the text,\n",
    "        apply the sentiemnt analyzer, and write chunks to \n",
    "        the sentiment table\n",
    "    :param chunksize: size of chunks\n",
    "    \"\"\"\n",
    "    sql = ''' SELECT * FROM text '''\n",
    "    reader = pd.read_sql_query(sql,\n",
    "                               conn,\n",
    "                               chunksize=chunksize)\n",
    "    for i, df in enumerate(tqdm(reader)):\n",
    "        df = gen_sentiment(df, 'text', 'all')\n",
    "        df = gen_sentiment(df, 'text_clean', 'clean')\n",
    "        df.drop('text', axis=1, inplace=True)\n",
    "        df.drop('text_clean', axis=1, inplace=True)\n",
    "        df.drop('probable_spam', axis=1, inplace=True)\n",
    "        if i == 0:\n",
    "            df.to_sql('sentiment', conn, if_exists='replace', index=False)\n",
    "        else:\n",
    "            df.to_sql('sentiment', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_family(df, i):\n",
    "    # group\n",
    "    df = df[['family_id','text_clean']].groupby(['family_id'])['text_clean'].apply(' '.join)\n",
    "    df = df.to_frame()\n",
    "    df.reset_index(inplace=True)\n",
    "    # create sentiment\n",
    "    df = gen_sentiment(df, 'text_clean', 'clean')\n",
    "    # drop extra columns\n",
    "    df.drop('text_clean', axis=1, inplace=True)\n",
    "    # write to database\n",
    "    if i == 0:\n",
    "        df.to_sql('sentiment_family', conn, if_exists='replace', index=False)\n",
    "    else:\n",
    "        df.to_sql('sentiment_family', conn, if_exists='append', index=False)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_family(chunksize):\n",
    "    \"\"\" read data in chunks from the table, format the text,\n",
    "        apply the sentiemnt analyzer, and write chunks to \n",
    "        the sentiment table grouped by the family_id\n",
    "    :param chunksize: size of chunks\n",
    "    \"\"\"\n",
    "    sql = '''\n",
    "        SELECT p.family_id, t.message_id, t.text_clean\n",
    "        FROM text AS t\n",
    "        LEFT JOIN posts AS p\n",
    "        WHERE t.message_id = p.message_id AND t.probable_spam = 0\n",
    "    '''\n",
    "    reader = pd.read_sql_query(sql,\n",
    "                               conn,\n",
    "                               chunksize=chunksize)\n",
    "    orphans = pd.DataFrame()\n",
    "    for i, chunk in enumerate(tqdm(reader)):\n",
    "        # concat orphans from last chunk\n",
    "        df = pd.concat((orphans, chunk))\n",
    "        # identify new orphans\n",
    "        last_val = df['family_id'].iloc[-1]\n",
    "        is_orphan = df['family_id'] == last_val\n",
    "        df, orphans = df[~is_orphan], df[is_orphan]\n",
    "        # sentiment\n",
    "        sentiment_family(df, i)\n",
    "    # process last orphan\n",
    "    if orphans.shape[0] > 0:\n",
    "        sentiment_family(orphans, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating the sentiment values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentiment(df, var, name):\n",
    "    \"\"\" apply the sentiment score to the input var\n",
    "    :param var: string name of column getting sentiment for\n",
    "    :param name: string variable suffix\n",
    "    :return score: a dictionary of scores (neg, neu, pos, compound)\n",
    "    \"\"\"\n",
    "    sentiment = df[var].apply(lambda x: sentiment_scores(x, analyzer))\n",
    "    name_neg = \"neg_sen_{}\".format(name)\n",
    "    name_neu = \"neu_sen_{}\".format(name)\n",
    "    name_pos = \"pos_sen_{}\".format(name)\n",
    "    name_com = \"com_sen_{}\".format(name)\n",
    "    df[name_neg] = sentiment.apply(lambda x: x.get('neg', 0))\n",
    "    df[name_neu] = sentiment.apply(lambda x: x.get('neu', 0))\n",
    "    df[name_pos] = sentiment.apply(lambda x: x.get('pos', 0))\n",
    "    df[name_com] = sentiment.apply(lambda x: x.get('compound', 0))\n",
    "    del sentiment\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(sentence, analyzer):\n",
    "    \"\"\" create sentiment scores with the VADER analyzer\n",
    "    :param sentence: sentence to create scores for\n",
    "    :param analyzer: VADER sentiment analyzer\n",
    "    :return score: a dictionary of scores (neg, neu, pos, compound)\n",
    "    \"\"\"\n",
    "    score = analyzer.polarity_scores(sentence)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()\n",
    "path_parent = p.parents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_db = str(path_parent / \"database\" / \"youbemom-merged.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "Note: cannot process all data at once, breaks the data into chunks and processes each bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection(path_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = get_size(conn)\n",
    "nchunks = 200\n",
    "chunksize = floor(size / nchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cbb9f7904c45fd9d3e3d26fb6fbdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "|          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_data_family(chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
